{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Next-Word Prediction Model with Google Universal Sentence Encoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JURASA/USE/blob/main/Next_Word_Prediction_Model_with_Google_Universal_Sentence_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zavljtkc2Xsv"
      },
      "source": [
        "# **Google drive for local storage**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj-H_oca3bNa",
        "outputId": "4ab454b3-db98-4e8b-d6d5-8de23cf00adf"
      },
      "source": [
        "# This cell will prompt an external url to accept permissions for Colab to access Google Drive\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount(\"/gdrive\")\r\n",
        "\r\n",
        "%ls"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "corpus.txt  \u001b[0m\u001b[01;34mNWP-USE\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  vocabulary.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_qUksc3r3KX"
      },
      "source": [
        "# **Import ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d9_NB3vpwG8"
      },
      "source": [
        "# Getting all required libraries\r\n",
        "\r\n",
        "import os\r\n",
        "import re\r\n",
        "import gdown\r\n",
        "import numpy\r\n",
        "import string\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import tensorflow as tf\r\n",
        "from absl import logging\r\n",
        "import tensorflow_hub as hub\r\n",
        "from tensorflow import keras\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.models import Sequential\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "from keras.layers.recurrent import LSTM\r\n",
        "from keras.layers import Dense, Activation\r\n",
        "from keras.callbacks import LambdaCallback\r\n",
        "from keras.utils.data_utils import get_file\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHNTF6AHsQUL"
      },
      "source": [
        "## **Data preparation - _Generating Corpus_**\r\n",
        "The Dataset is based on a Tensorflow tutorial from Stanford, so all predicted words will be based on Deep learning and Machine learning terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb2U0kS38MRf",
        "outputId": "946406dd-c4ef-4ddb-c174-3b5f9f6407a2"
      },
      "source": [
        "# Download data from Google drive\r\n",
        "\r\n",
        "'''\r\n",
        "ORIGINAL DATASET URL:\r\n",
        "    https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt\r\n",
        "\r\n",
        "'''\r\n",
        "\r\n",
        "url = ' https://drive.google.com/uc?id=1YTBR7FiXssaKXHhOZbUbwoWw6jzQxxKW'\r\n",
        "output = 'corpus.txt'\r\n",
        "gdown.download(url, output, quiet=False)\r\n",
        "\r\n",
        "# Read local file from directory and clean contents\r\n",
        "with open('corpus.txt') as subject:\r\n",
        "  cache = subject.readlines()\r\n",
        "translator = str.maketrans('', '', string.punctuation) # Remove punctuation\r\n",
        "lines = [doc.lower().translate(translator) for doc in cache] # Switch to lower case"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From:  https://drive.google.com/uc?id=1YTBR7FiXssaKXHhOZbUbwoWw6jzQxxKW\n",
            "To: /content/corpus.txt\n",
            "7.55MB [00:00, 184MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKFp-u8951-g"
      },
      "source": [
        "# Generating a list of single words, the \"vocabulary\"\r\n",
        "vocabulary = list(set(' '.join(lines).replace('\\n','').split(' ')))\r\n",
        "primary_store = {}\r\n",
        "for strings, texts in enumerate(vocabulary):\r\n",
        "  primary_store[texts] = strings"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18Aw7bxIHLrW"
      },
      "source": [
        "# Splitting data into Train sets and test sets\r\n",
        "\r\n",
        "X = [] \r\n",
        "y = []\r\n",
        "\r\n",
        "for c in lines:\r\n",
        "  xxxx = c.replace('\\n','').split(' ')\r\n",
        "  X.append(' '.join(xxxx[:-1])) # X from the corpus\r\n",
        "\r\n",
        "  yyyy = [0 for i in range(len(vocabulary))] # Generate Y from the Vocabulary\r\n",
        "  # yyyy[primary_store[xxxx[-1]]] = 1\r\n",
        "  yyyy[primary_store[xxxx[-1]]] = 1\r\n",
        "  y.append(yyyy)\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\r\n",
        "y_test = numpy.array(y_test)\r\n",
        "y_train = numpy.array(y_train)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-ZoLdTpMBtg"
      },
      "source": [
        "## **Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLwEZ5NzMA20"
      },
      "source": [
        "# Import the Universal Sentence Encoder's TF Hub module \r\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"  \r\n",
        "appreciate = hub.load(module_url)\r\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFVpJJ-kOgJK"
      },
      "source": [
        "# Implementing the Universal Search Encoder\r\n",
        "X_train = appreciate(X_train)\r\n",
        "X_test = appreciate(X_test)\r\n",
        "X_train = X_train.numpy()\r\n",
        "X_test = X_test.numpy()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk1zrMu1Q_QW"
      },
      "source": [
        "# **Buildinging the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE2cmc3eQ9qk",
        "outputId": "118878e6-6faa-4d98-c85e-cf63a04bc320"
      },
      "source": [
        "model = Sequential()\r\n",
        "# model.add(LSTM(units=100, input_shape=[512]))\r\n",
        "model.add(Dense(512, input_shape=[512], activation = 'relu'))\r\n",
        "model.add(Dense(units=len(vocabulary), activation = 'softmax'))\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2694)              2761350   \n",
            "=================================================================\n",
            "Total params: 3,286,662\n",
            "Trainable params: 3,286,662\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12j8kuJWUAKJ",
        "outputId": "c757733d-a1ef-4c80-8888-fb4e23356a3a"
      },
      "source": [
        "# Training the model. \r\n",
        "model.fit(X_train, y_train, batch_size=2048, shuffle=True, epochs=30, validation_data=(X_test, y_test), callbacks=[LambdaCallback()])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0458 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 1.0000\n",
            "Epoch 2/30\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0439 - acc: 1.0000 - val_loss: 0.0449 - val_acc: 1.0000\n",
            "Epoch 3/30\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0421 - acc: 1.0000 - val_loss: 0.0431 - val_acc: 1.0000\n",
            "Epoch 4/30\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0405 - acc: 1.0000 - val_loss: 0.0414 - val_acc: 1.0000\n",
            "Epoch 5/30\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0389 - acc: 1.0000 - val_loss: 0.0399 - val_acc: 1.0000\n",
            "Epoch 6/30\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0375 - acc: 1.0000 - val_loss: 0.0385 - val_acc: 1.0000\n",
            "Epoch 7/30\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0362 - acc: 1.0000 - val_loss: 0.0371 - val_acc: 1.0000\n",
            "Epoch 8/30\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0349 - acc: 1.0000 - val_loss: 0.0358 - val_acc: 1.0000\n",
            "Epoch 9/30\n",
            "3/3 [==============================] - 0s 178ms/step - loss: 0.0337 - acc: 1.0000 - val_loss: 0.0346 - val_acc: 1.0000\n",
            "Epoch 10/30\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0326 - acc: 1.0000 - val_loss: 0.0334 - val_acc: 1.0000\n",
            "Epoch 11/30\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0315 - acc: 1.0000 - val_loss: 0.0323 - val_acc: 1.0000\n",
            "Epoch 12/30\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0305 - acc: 1.0000 - val_loss: 0.0313 - val_acc: 1.0000\n",
            "Epoch 13/30\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0295 - acc: 1.0000 - val_loss: 0.0303 - val_acc: 1.0000\n",
            "Epoch 14/30\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0286 - acc: 1.0000 - val_loss: 0.0294 - val_acc: 1.0000\n",
            "Epoch 15/30\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.0278 - acc: 1.0000 - val_loss: 0.0286 - val_acc: 1.0000\n",
            "Epoch 16/30\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0269 - acc: 1.0000 - val_loss: 0.0277 - val_acc: 1.0000\n",
            "Epoch 17/30\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.0262 - acc: 1.0000 - val_loss: 0.0269 - val_acc: 1.0000\n",
            "Epoch 18/30\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0254 - acc: 1.0000 - val_loss: 0.0261 - val_acc: 1.0000\n",
            "Epoch 19/30\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0247 - acc: 1.0000 - val_loss: 0.0254 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.0240 - acc: 1.0000 - val_loss: 0.0247 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.0234 - acc: 1.0000 - val_loss: 0.0241 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0227 - acc: 1.0000 - val_loss: 0.0234 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0222 - acc: 1.0000 - val_loss: 0.0228 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0216 - acc: 1.0000 - val_loss: 0.0222 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0210 - acc: 1.0000 - val_loss: 0.0217 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0211 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0200 - acc: 1.0000 - val_loss: 0.0206 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.0201 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.0196 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.0192 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f660a569320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vzq-1bH0oOv"
      },
      "source": [
        "#**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Jy3Hbi0nlK"
      },
      "source": [
        "# Create function to predict and show detailed output\r\n",
        "def next_word(collection=[], extent=1):\r\n",
        "\r\n",
        "  for item in collection:\r\n",
        "    text = item\r\n",
        "    for i in range(extent):\r\n",
        "      prediction = model.predict(x=appreciate([item]).numpy())\r\n",
        "      idx = np.argmax(prediction[-1])\r\n",
        "      item += ' ' + vocabulary[idx]\r\n",
        "      \r\n",
        "      print(text + ' --> ' + item + '\\nNEXT WORD: ' + item.split(' ')[-1] + '\\n')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6_rGHmFYMKD",
        "outputId": "cec5adad-94a2-4d82-cc48-506a44bc1f67"
      },
      "source": [
        "# Tests\r\n",
        "single_text = ['we note that']\r\n",
        "next_word(single_text)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we note that --> we note that experiments\n",
            "NEXT WORD: experiments\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tZjEA0MX5pr",
        "outputId": "d871b1fd-ac4d-4dcf-b2a3-80312901470a"
      },
      "source": [
        "# Testing on a collection of words\r\n",
        "\r\n",
        "text_collection = ['deep convolutional', 'simple and effective', 'complex hilbert', 'a', 'there is', 'that party was']\r\n",
        "next_word(text_collection)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deep convolutional --> deep convolutional networks\n",
            "NEXT WORD: networks\n",
            "\n",
            "simple and effective --> simple and effective estimators\n",
            "NEXT WORD: estimators\n",
            "\n",
            "complex hilbert --> complex hilbert relu\n",
            "NEXT WORD: relu\n",
            "\n",
            "a --> a accuracy\n",
            "NEXT WORD: accuracy\n",
            "\n",
            "there is --> there is accuracy\n",
            "NEXT WORD: accuracy\n",
            "\n",
            "that party was --> that party was networks\n",
            "NEXT WORD: networks\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJTRmQh6aHSV",
        "outputId": "1a8deab9-6dcf-4ef7-c059-441b2aba656a"
      },
      "source": [
        "# Storing data\r\n",
        "vocabulary = numpy.array(vocabulary)\r\n",
        "numpy.save('./vocabulary.npy', vocabulary)\r\n",
        "model.save('./NWP-USE')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./NWP-USE/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./NWP-USE/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxCntzMTaXQ3"
      },
      "source": [
        "##                                                                  END OF NOTEBOOK"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}